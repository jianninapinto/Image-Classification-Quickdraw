{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "\n",
        "\n",
        "# Hyperparameter Tuning Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
        "\n",
        "# Gridsearch Hyperparameters\n",
        "\n",
        "In the guided project, you learned how to use sklearn's `GridsearchCV` and `keras-tuner` libraries to tune the hyperparameters of a neural network model. For your module project you'll continue using these two libraries, however we are going to make things a little more interesting for you. \n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n",
        "\n",
        "\n",
        "\n",
        "**Don't forget to switch to GPU on Colab!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7oEgGCV3_hY"
      },
      "source": [
        "## 0.1 Imports and installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxctNMPb7mNY",
        "outputId": "0045c1ed-42b2-427f-f2df-abed24117b73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.0-py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (7.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (23.0)\n",
            "Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (2.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (2.25.1)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (15.0.6.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.51.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (4.5.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.22.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (3.19.6)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.11.2)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (23.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (63.4.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (3.1.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython->keras-tuner) (2.0.10)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipython->keras-tuner) (5.7.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0->keras-tuner) (0.40.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython->keras-tuner) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (0.2.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (2.16.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (3.2.2)\n",
            "Installing collected packages: kt-legacy, jedi, keras-tuner\n",
            "Successfully installed jedi-0.18.2 keras-tuner-1.3.0 kt-legacy-1.0.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-96cd3167a526>:25: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner.tuners import RandomSearch, BayesianOptimization\n"
          ]
        }
      ],
      "source": [
        "# native python libraries imports \n",
        "import math\n",
        "from time import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn imports \n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# keras imports \n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.activations import relu, sigmoid\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "# required for compatibility between sklearn and keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# install keras-tuner\n",
        "!pip install keras-tuner\n",
        "from kerastuner.tuners import RandomSearch, BayesianOptimization\n",
        "from kerastuner.engine.hyperparameters import HyperParameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMBS8CRBzYqB"
      },
      "source": [
        "## 0.2 Load quickdraw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr8w6IX37mNa"
      },
      "outputs": [],
      "source": [
        "# Fill out the doc string\n",
        "\n",
        "def load_quickdraw10():\n",
        "    \"\"\"\n",
        "    Loads the Quickdraw-10 dataset and splits it into training and testing sets.\n",
        "\n",
        "    The Quickdraw-10 dataset consists of drawings of 10 different classes:\n",
        "    - apple\n",
        "    - anvil\n",
        "    - airplane\n",
        "    - banana\n",
        "    - The Eiffel Tower\n",
        "    - The Mona Lisa\n",
        "    - The Great Wall of China\n",
        "    - alarm clock\n",
        "    - ant\n",
        "    - asparagus\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      A tuple of four Numpy arrays: (X_train, X_test, y_train, y_test)\n",
        "\n",
        "      X_train : A 2D Numpy array of shape (num_train_samples, num_features) representing the\n",
        "             training data, where each row corresponds to a drawing and each column\n",
        "             corresponds to a feature.\n",
        "\n",
        "      X_test: A 2D Numpy array of shape (num_test_samples, num_features) representing the\n",
        "            testing data, where each row corresponds to a drawing and each column\n",
        "            corresponds to a feature.\n",
        "            \n",
        "      y_train: A 1D Numpy array of shape (num_train_samples,) representing the labels\n",
        "             of the training data.\n",
        "             \n",
        "      y_test: A 1D Numpy array of shape (num_test_samples,) representing the labels\n",
        "            of the testing data.\n",
        "    \"\"\"\n",
        "    \n",
        "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
        "    \n",
        "    path_to_zip = get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
        "\n",
        "    data = np.load(path_to_zip)\n",
        "    \n",
        "    # normalize your image data\n",
        "    max_pixel_value = 255\n",
        "    X = data['arr_0']/max_pixel_value\n",
        "    Y = data['arr_1']\n",
        "        \n",
        "    return train_test_split(X, Y, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjU5nY3e7mNc",
        "outputId": "f1f41a9d-484f-45b3-b808-519251a50504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\n",
            "25421363/25421363 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = load_quickdraw10()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkvBPoUy7mNd",
        "outputId": "146c4023-753b-430e-f985-1e8b82ce765f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(75000, 784)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4dx6VA07mNe",
        "outputId": "98559b6b-6a96-4e94-d575-e80dfbcd9e0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(75000,)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXsWtj8Z7mNf"
      },
      "source": [
        "_____\n",
        "\n",
        "# Experiment 1\n",
        "\n",
        "## Tune Hyperperameters using Enhanced GridsearchCV \n",
        "\n",
        "We are going to use GridsearchCV again to tune a deep learning model; however, we are going to add some additional functionality to our gridsearch. \n",
        "\n",
        "Specifically, we are going to automate the generation of how many nodes to use in a layer and how many layers to use in a model! \n",
        "\n",
        "By the way, yes, there is a function within a function. Try to not let that bother you. An alternative to this would be to create a class. If you're up for the challenge give it a shot. However, consider this a stretch goal that you come back to after you finish going through this assignment. \n",
        "\n",
        "\n",
        "### Objective \n",
        "\n",
        "The objective of this experiment is to show you how to automate the generation of layers and layer nodes for the purposes of gridsearch. <br>\n",
        "Up until now, we've been manually selecting the number of layers and layer nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USXjs7Hk71Hy"
      },
      "outputs": [],
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(n_layers,  first_layer_nodes, last_layer_nodes, act_funct =\"relu\", negative_node_incrementation=True):\n",
        "    \"\"\"\"\n",
        "    Returns a compiled keras model \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_layers: int \n",
        "        number of hidden layers in model \n",
        "        To be clear, this excludes the input and output layer.\n",
        "        \n",
        "    first_layer_nodes: int\n",
        "        Number of nodes in the first hidden layer \n",
        "\n",
        "    last_layer_nodes: int\n",
        "        Number of nodes in the last hidden layer (this is the layer just prior to the output layer)\n",
        "        \n",
        "     act_funct: string \n",
        "         Name of activation function to use in hidden layers (this excludes the output layer)\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    model: keras object \n",
        "    \"\"\"\n",
        "    \n",
        "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
        "        \"\"\"\n",
        "        Generates and returns the number of nodes in each hidden layer. \n",
        "        To be clear, this excludes the input and output layer. \n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Number of nodes in each layer is linearly incremented. \n",
        "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers: int\n",
        "            Number of hidden layers\n",
        "            This values should be 2 or greater \n",
        "\n",
        "        first_layer_nodes: int\n",
        "\n",
        "        last_layer_nodes: int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: list of ints\n",
        "            Contains number of nodes for each layer \n",
        "        \"\"\"\n",
        "\n",
        "        # throws an error if n_layers is less than 2 \n",
        "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
        "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
        "        # when set to True number of nodes are decreased for subsequent layers \n",
        "        # NOTE: the order of the number of nodes doesn't matter\n",
        "        if negative_node_incrementation:\n",
        "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            \n",
        "        # when set to False number of nodes are increased for subsequent layers\n",
        "        else:\n",
        "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
        "            nodes_increment = (first_layer_nodes - last_layer_nodes)/ (n_layers-1)\n",
        "\n",
        "        nodes = first_layer_nodes\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "\n",
        "            layers.append(math.ceil(nodes))\n",
        "\n",
        "            # increment nodes for next layer \n",
        "            nodes = nodes + nodes_increment\n",
        "\n",
        "        return layers\n",
        "    \n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "    \n",
        "    # create all hidden layers\n",
        "    for i in range(1, n_layers+1):\n",
        "        if i==1:\n",
        "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=act_funct))\n",
        "        else:\n",
        "            model.add(Dense(n_nodes[i-1], activation=act_funct))\n",
        "            \n",
        "            \n",
        "    # output layer \n",
        "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
        "                    activation='softmax')) # use softmax for a label set greater than 2            \n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', \n",
        "                  optimizer='adam', # adam is a good default optimizer \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # do not include model.fit() inside the create_model function\n",
        "    # KerasClassifier is expecting a complied model \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO-x0nqt7mNh"
      },
      "source": [
        "## 1.1 Explore `create_model`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1hnjQHKW19w"
      },
      "source": [
        "The helper function `gen_layer_nodes()` which is contained inside `create_model()` <br>\n",
        "returns a list containing the number of nodes for each successive layer.<br>\n",
        "\n",
        "Let's check that `gen_layer_nodes()` behaves as expected. <br>\n",
        "In other words, we'll perform a **Unit Test!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiPXu0p_Qco_"
      },
      "outputs": [],
      "source": [
        "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
        "        \"\"\"\n",
        "        Generates and returns the number of nodes in each hidden layer. \n",
        "        To be clear, this excludes the input and output layer. \n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Number of nodes in each layer is linearly incremented. \n",
        "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers: int\n",
        "            Number of hidden layers\n",
        "            This values should be 2 or greater \n",
        "\n",
        "        first_layer_nodes: int\n",
        "\n",
        "        last_layer_nodes: int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: list of ints\n",
        "            Contains number of nodes for each layer \n",
        "        \"\"\"\n",
        "\n",
        "        # throws an error if n_layers is less than 2 \n",
        "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
        "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
        "        # when set to True number of nodes are decreased for subsequent layers \n",
        "        # NOTE: the order of the number of nodes doesn't matter\n",
        "        if negative_node_incrementation:\n",
        "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            #print(f'nodes increment = {nodes_increment}')\n",
        "            \n",
        "        # when set to False number of nodes are increased for subsequent layers\n",
        "        else:\n",
        "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            #print(f'nodes increment = {nodes_increment}')\n",
        "\n",
        "        nodes = first_layer_nodes\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "\n",
        "            layers.append(math.ceil(nodes))\n",
        "\n",
        "            # increment nodes for next layer \n",
        "            nodes = nodes + nodes_increment\n",
        "\n",
        "        return layers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj3MrB6jXUMG"
      },
      "source": [
        "### `negative_node_incrementation = True`\n",
        "For this case we want the number of nodes to _decrease_ by a constant number for successive layers. <br>So `first_layer_nodes` must be _larger_ than `last_layer_nodes` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m4jRNllXPPG",
        "outputId": "8f5c7edf-a2ed-402b-9994-25eda02cfa62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nodes in successive layers: [500, 400, 300, 200, 100]\n"
          ]
        }
      ],
      "source": [
        "n_layers = 5\n",
        "first_layer_nodes = 500\n",
        "last_layer_nodes = 100\n",
        "negative_node_incrementation = True\n",
        "n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "print(f'Number of nodes in successive layers: {n_nodes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttkaf3g9XhGr"
      },
      "source": [
        "### `negative_node_incrementation = False`\n",
        "For this case we want the number of nodes to _increase_ by a constant number for successive layers. <br>So `first_layer_nodes` must be _smaller_ than `last_layer_nodes` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fkrMS8bXQUo",
        "outputId": "3b5ab1db-7a87-4616-b89d-5b67d33e8b11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nodes in successive layers: [100, 200, 300, 400, 500]\n"
          ]
        }
      ],
      "source": [
        "n_layers = 5\n",
        "first_layer_nodes = 100\n",
        "last_layer_nodes = 500\n",
        "negative_node_incrementation = False\n",
        "n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "print(f'Number of nodes in successive layers: {n_nodes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHuB-bm5Wkpq"
      },
      "source": [
        "### OK, the Unit Test is passed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO3AjVWOZ6SA"
      },
      "source": [
        "### Let's build a few models<br> \n",
        "in order to understand how `create_model()` works in practice. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95E85Ug07mNh"
      },
      "source": [
        "### Build a model, setting `negative_node_incrementation = True` \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "x_1REOCY7mNi",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5dcf5c585f07629a03086cf57ba53615",
          "grade": false,
          "grade_id": "cell-86d63e89a21223de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "# YOUR CODE HERE\n",
        "model = create_model(n_layers=10,\n",
        "                     first_layer_nodes = 500,\n",
        "                     last_layer_nodes = 100, # this parameter is a limit when negative_node_incrementation=False\n",
        "                     act_funct = \"relu\",\n",
        "                     negative_node_incrementation = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYMwZQ7k7mNi",
        "outputId": "c614ade3-454c-4331-a969-d8219c343727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 500)               392500    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 456)               228456    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 412)               188284    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 367)               151571    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 323)               118864    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 278)               90072     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 234)               65286     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 189)               44415     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 145)               27550     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 100)               14600     \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,322,608\n",
            "Trainable params: 1,322,608\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values. \n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUc0jfnRm-uh"
      },
      "source": [
        "### Build a model, setting `negative_node_incrementation = False` \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 100`\n",
        "- Set `last_layer_nodes = 500`\n",
        "- Set `act_funct = \"relu\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "3_-kqHQtm-ui",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5dcf5c585f07629a03086cf57ba53615",
          "grade": false,
          "grade_id": "cell-86d63e89a21223de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "# YOUR CODE HERE\n",
        "model = create_model(n_layers=10,\n",
        "                     first_layer_nodes = 500,\n",
        "                     last_layer_nodes = 100, # this parameter is a limit when negative_node_incrementation=False\n",
        "                     act_funct = \"relu\",\n",
        "                     negative_node_incrementation = False)\n",
        "\n",
        "# raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piboKWsNm-uj",
        "outputId": "fee275b4-08b5-4d3e-82d2-9e47f56e3546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_11 (Dense)            (None, 500)               392500    \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 545)               273045    \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 589)               321594    \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 634)               374060    \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 678)               430530    \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 723)               490917    \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 767)               555308    \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 812)               623616    \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 856)               695928    \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 901)               772157    \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 10)                9020      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,938,675\n",
            "Trainable params: 4,938,675\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values. \n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBH7AR9p0OXi"
      },
      "source": [
        "## 1.2 Create a grid search using `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veloj7Nnlttf"
      },
      "source": [
        "### Hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e2lhZqP7mNn"
      },
      "outputs": [],
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'n_layers': [2, 3],\n",
        "              'epochs': [3], \n",
        "              \"first_layer_nodes\": [500, 300],\n",
        "              \"last_layer_nodes\": [100, 50]\n",
        "             }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ks_MLPB7mNn",
        "outputId": "18c7e308-e5ed-4dc0-8072-f1b918d37b52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-22e3fde777af>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(create_model)\n"
          ]
        }
      ],
      "source": [
        "model = KerasClassifier(create_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8GKbLJ_7mNn",
        "outputId": "dd0c088d-ea9e-47f9-e7da-ea3703c421bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 10s 3ms/step - loss: 0.6471 - accuracy: 0.8028\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4421 - accuracy: 0.8663\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3550 - accuracy: 0.8916\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4703 - accuracy: 0.8616\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6467 - accuracy: 0.8026\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4383 - accuracy: 0.8686\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3511 - accuracy: 0.8920\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4642 - accuracy: 0.8629\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6538 - accuracy: 0.8022\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4435 - accuracy: 0.8673\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3554 - accuracy: 0.8908\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4534 - accuracy: 0.8678\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6525 - accuracy: 0.7981\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4455 - accuracy: 0.8642\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3615 - accuracy: 0.8877\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.4540 - accuracy: 0.8631\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6482 - accuracy: 0.8001\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4474 - accuracy: 0.8636\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3594 - accuracy: 0.8905\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4587 - accuracy: 0.8660\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6466 - accuracy: 0.8014\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4399 - accuracy: 0.8661\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 4ms/step - loss: 0.3615 - accuracy: 0.8879\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4488 - accuracy: 0.8644\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6562 - accuracy: 0.8024\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4499 - accuracy: 0.8650\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3628 - accuracy: 0.8904\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5057 - accuracy: 0.8483\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6575 - accuracy: 0.7996\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4482 - accuracy: 0.8649\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3657 - accuracy: 0.8892\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4590 - accuracy: 0.8664\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6634 - accuracy: 0.7990\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4529 - accuracy: 0.8628\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3679 - accuracy: 0.8876\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4644 - accuracy: 0.8620\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6509 - accuracy: 0.7998\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4412 - accuracy: 0.8654\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 4ms/step - loss: 0.3612 - accuracy: 0.8882\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4762 - accuracy: 0.8604\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6522 - accuracy: 0.7996\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4435 - accuracy: 0.8650\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3617 - accuracy: 0.8888\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4618 - accuracy: 0.8624\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6527 - accuracy: 0.8010\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4439 - accuracy: 0.8638\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3595 - accuracy: 0.8900\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4613 - accuracy: 0.8648\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6724 - accuracy: 0.7959\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4611 - accuracy: 0.8609\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3776 - accuracy: 0.8842\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.4811 - accuracy: 0.8596\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 11s 6ms/step - loss: 0.6676 - accuracy: 0.7979\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4537 - accuracy: 0.8637\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3706 - accuracy: 0.8884\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.4677 - accuracy: 0.8596\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6678 - accuracy: 0.7981\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4514 - accuracy: 0.8634\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3684 - accuracy: 0.8883\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4551 - accuracy: 0.8646\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6664 - accuracy: 0.7944\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4594 - accuracy: 0.8602\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3796 - accuracy: 0.8836\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4699 - accuracy: 0.8610\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6628 - accuracy: 0.7954\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4606 - accuracy: 0.8596\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3792 - accuracy: 0.8838\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4660 - accuracy: 0.8622\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6647 - accuracy: 0.7959\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4571 - accuracy: 0.8606\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3791 - accuracy: 0.8838\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4737 - accuracy: 0.8581\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6792 - accuracy: 0.7958\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4607 - accuracy: 0.8620\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3773 - accuracy: 0.8864\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4749 - accuracy: 0.8612\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6843 - accuracy: 0.7935\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4668 - accuracy: 0.8588\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3800 - accuracy: 0.8852\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4746 - accuracy: 0.8613\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6826 - accuracy: 0.7964\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4651 - accuracy: 0.8598\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3811 - accuracy: 0.8841\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4781 - accuracy: 0.8599\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6705 - accuracy: 0.7948\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4589 - accuracy: 0.8598\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3787 - accuracy: 0.8830\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4669 - accuracy: 0.8626\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6692 - accuracy: 0.7938\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4589 - accuracy: 0.8616\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3817 - accuracy: 0.8827\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4646 - accuracy: 0.8600\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6669 - accuracy: 0.7949\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4607 - accuracy: 0.8572\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3807 - accuracy: 0.8823\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4546 - accuracy: 0.8648\n",
            "Epoch 1/3\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 0.5960 - accuracy: 0.8171\n",
            "Epoch 2/3\n",
            "2344/2344 [==============================] - 8s 3ms/step - loss: 0.4193 - accuracy: 0.8720\n",
            "Epoch 3/3\n",
            "2344/2344 [==============================] - 9s 4ms/step - loss: 0.3481 - accuracy: 0.8937\n",
            "Best: 0.8644933303197225 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8641066551208496, Stdev: 0.0026356298629851243 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8644933303197225, Stdev: 0.0012120749193442713 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8589066664377848, Stdev: 0.007698400098734658 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8625466624895731, Stdev: 0.0017978572168788722 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8612800041834513, Stdev: 0.0023476502910666805 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.860426684220632, Stdev: 0.0017301451798566938 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8608133395512899, Stdev: 0.0006335671304406922 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8624666730562845, Stdev: 0.001979702515209157 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "CPU times: user 8min 32s, sys: 47.4 s, total: 9min 20s\n",
            "Wall time: 9min 32s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfH6okqe7mNo"
      },
      "outputs": [],
      "source": [
        "best_model = grid_result.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Inlda_0w7mNo",
        "outputId": "5aacb698-b067-45c1-9562-ad2b2b9ea8b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'epochs': 3,\n",
              " 'first_layer_nodes': 500,\n",
              " 'last_layer_nodes': 100,\n",
              " 'n_layers': 3,\n",
              " 'build_fn': <function __main__.create_model(n_layers, first_layer_nodes, last_layer_nodes, act_funct='relu', negative_node_incrementation=True)>}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model.get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrs3Yib17mNl"
      },
      "source": [
        "Ok, now that we've played around a bit with  `create_model`, let's build a  simpler model that we'll use to run gridsearches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvegpS1-5yYX"
      },
      "source": [
        "### Build model\n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 2` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-NcKYRr5yYX",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4ca6c5e51302fd10",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "###BEGIN SOLUTION\n",
        "# use create_model to create a model \n",
        "model = create_model(n_layers = 2,\n",
        "                     first_layer_nodes = 500,\n",
        "                     last_layer_nodes = 100,\n",
        "                     act_funct = \"relu\",\n",
        "                     negative_node_incrementation= True)\n",
        "\n",
        "###END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICLd6cYN5yYY",
        "outputId": "fa5d27cc-abd1-4183-85f5-03c5ce1b5006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_110 (Dense)           (None, 500)               392500    \n",
            "                                                                 \n",
            " dense_111 (Dense)           (None, 100)               50100     \n",
            "                                                                 \n",
            " dense_112 (Dense)           (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 443,610\n",
            "Trainable params: 443,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwY6GFo85yYY"
      },
      "outputs": [],
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'n_layers': [2, 3],\n",
        "              'epochs': [3], \n",
        "              \"first_layer_nodes\": [500, 300],\n",
        "              \"last_layer_nodes\": [100, 50]\n",
        "             }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a0iHBqJ5yYY",
        "outputId": "cd30ce5a-8a94-45b9-b81f-2f5533c06d09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-22e3fde777af>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(create_model)\n"
          ]
        }
      ],
      "source": [
        "model = KerasClassifier(create_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxpuM3g15yYZ",
        "outputId": "a283e24f-cd3e-4889-952c-0aa136a2290c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6481 - accuracy: 0.8040\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4419 - accuracy: 0.8660\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3565 - accuracy: 0.8913\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4632 - accuracy: 0.8628\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6438 - accuracy: 0.8046\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4402 - accuracy: 0.8661\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3578 - accuracy: 0.8916\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4738 - accuracy: 0.8622\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6436 - accuracy: 0.8057\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4370 - accuracy: 0.8673\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3523 - accuracy: 0.8933\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4539 - accuracy: 0.8658\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6555 - accuracy: 0.8001\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4411 - accuracy: 0.8646\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3610 - accuracy: 0.8888\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4730 - accuracy: 0.8620\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6493 - accuracy: 0.8018\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4448 - accuracy: 0.8638\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3625 - accuracy: 0.8886\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4468 - accuracy: 0.8644\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6543 - accuracy: 0.7981\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4449 - accuracy: 0.8645\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3634 - accuracy: 0.8879\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4602 - accuracy: 0.8622\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6658 - accuracy: 0.7998\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4504 - accuracy: 0.8634\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3611 - accuracy: 0.8899\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4833 - accuracy: 0.8602\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6591 - accuracy: 0.8016\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4486 - accuracy: 0.8638\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3640 - accuracy: 0.8909\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4762 - accuracy: 0.8613\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6634 - accuracy: 0.8011\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4481 - accuracy: 0.8659\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3635 - accuracy: 0.8895\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4633 - accuracy: 0.8648\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6546 - accuracy: 0.7977\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4453 - accuracy: 0.8644\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3633 - accuracy: 0.8890\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4680 - accuracy: 0.8637\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6567 - accuracy: 0.7983\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4430 - accuracy: 0.8650\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3591 - accuracy: 0.8907\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.4629 - accuracy: 0.8654\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6538 - accuracy: 0.7987\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4421 - accuracy: 0.8645\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3606 - accuracy: 0.8883\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4763 - accuracy: 0.8575\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6714 - accuracy: 0.7959\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4617 - accuracy: 0.8602\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3773 - accuracy: 0.8839\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4562 - accuracy: 0.8668\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6729 - accuracy: 0.7950\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4612 - accuracy: 0.8604\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3792 - accuracy: 0.8841\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4887 - accuracy: 0.8556\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6720 - accuracy: 0.7971\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4579 - accuracy: 0.8608\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3721 - accuracy: 0.8868\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4663 - accuracy: 0.8645\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6601 - accuracy: 0.7985\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4551 - accuracy: 0.8599\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3749 - accuracy: 0.8842\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.5070 - accuracy: 0.8539\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6607 - accuracy: 0.7988\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4530 - accuracy: 0.8623\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3754 - accuracy: 0.8847\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4788 - accuracy: 0.8569\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6646 - accuracy: 0.7955\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4574 - accuracy: 0.8604\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3739 - accuracy: 0.8851\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4723 - accuracy: 0.8610\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6804 - accuracy: 0.7944\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4668 - accuracy: 0.8586\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3798 - accuracy: 0.8841\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4871 - accuracy: 0.8578\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6789 - accuracy: 0.7941\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4680 - accuracy: 0.8599\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3840 - accuracy: 0.8844\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4680 - accuracy: 0.8605\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6855 - accuracy: 0.7932\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4683 - accuracy: 0.8596\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3841 - accuracy: 0.8839\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4928 - accuracy: 0.8544\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6729 - accuracy: 0.7945\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4558 - accuracy: 0.8602\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3775 - accuracy: 0.8845\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4683 - accuracy: 0.8621\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6676 - accuracy: 0.7931\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4525 - accuracy: 0.8615\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3723 - accuracy: 0.8860\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4660 - accuracy: 0.8605\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6720 - accuracy: 0.7940\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4597 - accuracy: 0.8591\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3799 - accuracy: 0.8837\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4573 - accuracy: 0.8628\n",
            "Epoch 1/3\n",
            "2344/2344 [==============================] - 9s 3ms/step - loss: 0.5988 - accuracy: 0.8186\n",
            "Epoch 2/3\n",
            "2344/2344 [==============================] - 8s 3ms/step - loss: 0.4145 - accuracy: 0.8759\n",
            "Epoch 3/3\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.3399 - accuracy: 0.8959\n",
            "Best: 0.8636266787846884 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8636266787846884, Stdev: 0.0015561840448859194 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.862880011399587, Stdev: 0.0011090032810340978 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8621333241462708, Stdev: 0.001964021840958209 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8622133334477743, Stdev: 0.0033955903059799454 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8622933427492777, Stdev: 0.004851321282962381 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8572799960772196, Stdev: 0.002901592820360614 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8575599988301595, Stdev: 0.002506322812131403 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8618266582489014, Stdev: 0.0009695756015812259 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "CPU times: user 8min 31s, sys: 48.4 s, total: 9min 19s\n",
            "Wall time: 9min 27s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIlpwjag5yYZ"
      },
      "outputs": [],
      "source": [
        "best_model = grid_result.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFvMxmr85yYZ",
        "outputId": "d78a92a3-d736-4035-9322-68e586cf3092"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'epochs': 3,\n",
              " 'first_layer_nodes': 500,\n",
              " 'last_layer_nodes': 100,\n",
              " 'n_layers': 2,\n",
              " 'build_fn': <function __main__.create_model(n_layers, first_layer_nodes, last_layer_nodes, act_funct='relu', negative_node_incrementation=True)>}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model.get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6azV65Nb7mNo"
      },
      "source": [
        "-----\n",
        "\n",
        "# Experiment 2: Run the Gridsearch Algorithms \n",
        "\n",
        "In this section, we are going to use the same model and dataset in order to benchmark 3 different gridsearch approaches: \n",
        "\n",
        "- Gridsearch\n",
        "- Random Search\n",
        "- Bayesian Optimization. \n",
        "\n",
        "\n",
        "Our goal in this experiment is two-fold. We want to see which approach \n",
        "\n",
        "- Scores the highest accuracy\n",
        "- Has the shortest run time \n",
        "\n",
        "We want to see how these 3 gridsearch approaches handle these trade-offs and to give you a sense of those trades offs.\n",
        "\n",
        "### Trade-offs\n",
        "\n",
        "`Gridsearch` will train a model on every single unique hyperparameter combination, this guarantees that you'll get the highest possible accuracy from your parameter set but your gridsearch might have a very long run-time. \n",
        "\n",
        "`Random Search` will randomly sample from your parameter set which, depending on how many samples, the run-time might be significantly cut down but you might or might not sample the parameters that correspond to the highest possible accuracies. \n",
        "\n",
        "`Bayesian Optimization` has a bit of intelligence built into its search algorithm but you do need to manually select some parameters which may greatly influence the model learning outcomes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X41u_hls7mNp"
      },
      "source": [
        "-------\n",
        "### Build our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ_uyKlj7mNp"
      },
      "outputs": [],
      "source": [
        "# because gridsearching can take a lot of time and we are bench marking 3 different approaches\n",
        "# let's build a simple model to minimize run time \n",
        "\n",
        "def build_model(hp):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complied keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(hp.get('learning_rate')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PYE7rTku7mNp",
        "outputId": "7255af1b-d620-4ed8-898d-a8e91db45b5c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'relu'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hp = HyperParameters()\n",
        "hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
        "hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqjp2kHD7mNu"
      },
      "source": [
        "---------\n",
        "## 2.1 Gridsearch Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsNW4rJp7mNu"
      },
      "source": [
        "### Populate a `sklearn` compatible parameter dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJQFKoyL7mNu"
      },
      "outputs": [],
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hyper_parameters = {\n",
        "    # BUG Fix: cast array as list otherwise GridSearchCV will throw error\n",
        "    \"units\": np.arange(32, 512, 32).tolist(),\n",
        "    \"learning_rate\": [1e-1, 1e-2, 1e-3],\n",
        "    \"activation\":[\"relu\", \"sigmoid\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcxV58iC7mNu",
        "outputId": "3893054e-b588-49a2-bc29-b7d7476d7589"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'units': [32,\n",
              "  64,\n",
              "  96,\n",
              "  128,\n",
              "  160,\n",
              "  192,\n",
              "  224,\n",
              "  256,\n",
              "  288,\n",
              "  320,\n",
              "  352,\n",
              "  384,\n",
              "  416,\n",
              "  448,\n",
              "  480],\n",
              " 'learning_rate': [0.1, 0.01, 0.001],\n",
              " 'activation': ['relu', 'sigmoid']}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hyper_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOoMg9Ao7mNv"
      },
      "source": [
        "### Build a `sklearn` compatible model function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZFVl-I-7mNv"
      },
      "outputs": [],
      "source": [
        "def build_model(units, learning_rate, activation):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complie keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units, activation=activation))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqYmn3QFsqZ_"
      },
      "source": [
        "### Apply the \"wrapper\" to make the model compatible with `sklearn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABSzrTrH7mNw",
        "outputId": "930ba480-f9f6-4a20-d025-b818be7aef6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-34-a93dc876c14b>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn = build_model)\n"
          ]
        }
      ],
      "source": [
        "model = KerasClassifier(build_fn = build_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTawllrN7mNw",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "b1b861bd-9625-479f-e13c-f58ffae0bdf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 90 candidates, totalling 270 fits\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.1231 - accuracy: 0.1895\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 2.0949 - accuracy: 0.1830\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9008 - accuracy: 0.2983\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9213 - accuracy: 0.3184\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.3256 - accuracy: 0.1087\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 2.3149 - accuracy: 0.1039\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9391 - accuracy: 0.3104\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0823 - accuracy: 0.2286\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0816 - accuracy: 0.2295\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0052 - accuracy: 0.2393\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1855 - accuracy: 0.1921\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1328 - accuracy: 0.1708\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9105 - accuracy: 0.3198\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.2749 - accuracy: 0.2653\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.8882 - accuracy: 0.3375\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9301 - accuracy: 0.2748\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9300 - accuracy: 0.2976\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9266 - accuracy: 0.2726\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9469 - accuracy: 0.3064\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9695 - accuracy: 0.2674\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.8931 - accuracy: 0.3467\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9379 - accuracy: 0.3007\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9106 - accuracy: 0.3432\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8662 - accuracy: 0.3147\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0534 - accuracy: 0.2656\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0558 - accuracy: 0.2390\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.8703 - accuracy: 0.3620\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7909 - accuracy: 0.3411\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9297 - accuracy: 0.3469\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.7971 - accuracy: 0.3348\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0748 - accuracy: 0.2527\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0300 - accuracy: 0.2451\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9852 - accuracy: 0.3482\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8361 - accuracy: 0.3092\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9902 - accuracy: 0.3195\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9845 - accuracy: 0.2294\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9968 - accuracy: 0.3407\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8428 - accuracy: 0.3456\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1816 - accuracy: 0.2447\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0229 - accuracy: 0.2209\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0545 - accuracy: 0.3187\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.9780 - accuracy: 0.2514\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0161 - accuracy: 0.3242\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.3056 - accuracy: 0.3127\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9648 - accuracy: 0.3365\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7753 - accuracy: 0.3162\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9974 - accuracy: 0.3262\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0446 - accuracy: 0.2102\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0789 - accuracy: 0.3132\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9878 - accuracy: 0.2528\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0101 - accuracy: 0.3831\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9400 - accuracy: 0.3097\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9899 - accuracy: 0.3558\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9806 - accuracy: 0.3472\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0202 - accuracy: 0.3300\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0020 - accuracy: 0.2374\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0044 - accuracy: 0.3610\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.9288 - accuracy: 0.2913\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9833 - accuracy: 0.3205\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0933 - accuracy: 0.2101\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9418 - accuracy: 0.3638\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 2.6504 - accuracy: 0.2626\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.2782 - accuracy: 0.2095\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1774 - accuracy: 0.1705\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.1155 - accuracy: 0.3241\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8960 - accuracy: 0.2748\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0595 - accuracy: 0.3245\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8098 - accuracy: 0.3093\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0356 - accuracy: 0.3923\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9639 - accuracy: 0.3653\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0937 - accuracy: 0.3230\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.6340 - accuracy: 0.2218\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0395 - accuracy: 0.3485\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8681 - accuracy: 0.3020\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0576 - accuracy: 0.3364\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.9239 - accuracy: 0.2612\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4125 - accuracy: 0.3285\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8769 - accuracy: 0.3084\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0401 - accuracy: 0.3459\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9152 - accuracy: 0.3296\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.1414 - accuracy: 0.3527\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8448 - accuracy: 0.3417\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.2821 - accuracy: 0.3086\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9132 - accuracy: 0.2879\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0439 - accuracy: 0.3564\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8817 - accuracy: 0.3321\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.1967 - accuracy: 0.3069\n",
            "782/782 [==============================] - 3s 2ms/step - loss: 2.0512 - accuracy: 0.2394\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1409 - accuracy: 0.3531\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1050 - accuracy: 0.2185\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.7940 - accuracy: 0.7563\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7136 - accuracy: 0.7848\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.7878 - accuracy: 0.7597\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6801 - accuracy: 0.7966\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7954 - accuracy: 0.7579\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7015 - accuracy: 0.7859\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7601 - accuracy: 0.7682\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.6888 - accuracy: 0.7938\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7585 - accuracy: 0.7694\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6682 - accuracy: 0.7964\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7497 - accuracy: 0.7693\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6477 - accuracy: 0.8044\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7497 - accuracy: 0.7727\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6884 - accuracy: 0.7944\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7623 - accuracy: 0.7690\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7114 - accuracy: 0.7939\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7447 - accuracy: 0.7746\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6521 - accuracy: 0.8090\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7413 - accuracy: 0.7762\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6620 - accuracy: 0.8059\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7445 - accuracy: 0.7730\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6421 - accuracy: 0.8055\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7415 - accuracy: 0.7750\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6465 - accuracy: 0.8119\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7425 - accuracy: 0.7738\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6839 - accuracy: 0.8089\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7464 - accuracy: 0.7741\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6702 - accuracy: 0.7982\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7399 - accuracy: 0.7788\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6443 - accuracy: 0.8087\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7374 - accuracy: 0.7768\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6475 - accuracy: 0.8074\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7381 - accuracy: 0.7750\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6869 - accuracy: 0.7947\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7466 - accuracy: 0.7748\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6794 - accuracy: 0.8036\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7450 - accuracy: 0.7766\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6654 - accuracy: 0.8055\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7436 - accuracy: 0.7740\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6800 - accuracy: 0.8000\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.7423 - accuracy: 0.7759\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6681 - accuracy: 0.7989\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7366 - accuracy: 0.7770\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7011 - accuracy: 0.8016\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7407 - accuracy: 0.7761\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6641 - accuracy: 0.8058\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7509 - accuracy: 0.7720\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6626 - accuracy: 0.8026\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7493 - accuracy: 0.7752\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7049 - accuracy: 0.7922\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7382 - accuracy: 0.7774\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6617 - accuracy: 0.8074\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7469 - accuracy: 0.7745\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6529 - accuracy: 0.8068\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7452 - accuracy: 0.7754\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6750 - accuracy: 0.8011\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7562 - accuracy: 0.7736\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6724 - accuracy: 0.8016\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7494 - accuracy: 0.7761\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7177 - accuracy: 0.7826\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7471 - accuracy: 0.7755\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6842 - accuracy: 0.8063\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7523 - accuracy: 0.7735\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6627 - accuracy: 0.8044\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7542 - accuracy: 0.7743\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7251 - accuracy: 0.7882\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7473 - accuracy: 0.7742\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6782 - accuracy: 0.7958\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7445 - accuracy: 0.7751\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7019 - accuracy: 0.7998\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7488 - accuracy: 0.7783\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7004 - accuracy: 0.7970\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7509 - accuracy: 0.7735\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6793 - accuracy: 0.7965\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7418 - accuracy: 0.7774\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6893 - accuracy: 0.7898\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7468 - accuracy: 0.7758\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6616 - accuracy: 0.7997\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7551 - accuracy: 0.7741\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7217 - accuracy: 0.7918\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7518 - accuracy: 0.7748\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6840 - accuracy: 0.7954\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7506 - accuracy: 0.7736\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6753 - accuracy: 0.7951\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7497 - accuracy: 0.7752\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6584 - accuracy: 0.8114\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7414 - accuracy: 0.7774\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6475 - accuracy: 0.8046\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.7431 - accuracy: 0.7785\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6679 - accuracy: 0.8032\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8791 - accuracy: 0.7401\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.7241 - accuracy: 0.7916\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8708 - accuracy: 0.7408\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.7380 - accuracy: 0.7869\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8621 - accuracy: 0.7478\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7199 - accuracy: 0.7852\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8088 - accuracy: 0.7608\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6518 - accuracy: 0.8117\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8022 - accuracy: 0.7631\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6574 - accuracy: 0.8092\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8114 - accuracy: 0.7594\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6603 - accuracy: 0.8048\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7683 - accuracy: 0.7721\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6223 - accuracy: 0.8179\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7714 - accuracy: 0.7722\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6288 - accuracy: 0.8148\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7704 - accuracy: 0.7717\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6142 - accuracy: 0.8182\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7414 - accuracy: 0.7825\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5972 - accuracy: 0.8238\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7432 - accuracy: 0.7778\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5940 - accuracy: 0.8256\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7485 - accuracy: 0.7770\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5905 - accuracy: 0.8236\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7285 - accuracy: 0.7832\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5754 - accuracy: 0.8298\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7309 - accuracy: 0.7843\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.5909 - accuracy: 0.8264\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.7268 - accuracy: 0.7844\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5698 - accuracy: 0.8322\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7181 - accuracy: 0.7867\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5835 - accuracy: 0.8261\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7111 - accuracy: 0.7876\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5714 - accuracy: 0.8310\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7158 - accuracy: 0.7877\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.5783 - accuracy: 0.8278\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7087 - accuracy: 0.7882\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5591 - accuracy: 0.8358\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7042 - accuracy: 0.7897\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.5667 - accuracy: 0.8333\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7012 - accuracy: 0.7915\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5567 - accuracy: 0.8365\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6934 - accuracy: 0.7945\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5418 - accuracy: 0.8424\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6934 - accuracy: 0.7927\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5482 - accuracy: 0.8404\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6947 - accuracy: 0.7934\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5665 - accuracy: 0.8328\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6909 - accuracy: 0.7954\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5491 - accuracy: 0.8386\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6921 - accuracy: 0.7933\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5614 - accuracy: 0.8336\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6939 - accuracy: 0.7937\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5538 - accuracy: 0.8329\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6863 - accuracy: 0.7965\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5631 - accuracy: 0.8334\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6865 - accuracy: 0.7948\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5408 - accuracy: 0.8432\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6842 - accuracy: 0.7956\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5359 - accuracy: 0.8412\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6776 - accuracy: 0.7986\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5391 - accuracy: 0.8412\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6788 - accuracy: 0.7983\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5765 - accuracy: 0.8288\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6797 - accuracy: 0.7960\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5536 - accuracy: 0.8353\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6785 - accuracy: 0.7972\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5327 - accuracy: 0.8450\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6797 - accuracy: 0.7977\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5596 - accuracy: 0.8361\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6769 - accuracy: 0.7964\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5325 - accuracy: 0.8429\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6753 - accuracy: 0.7979\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5286 - accuracy: 0.8429\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6721 - accuracy: 0.7988\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5295 - accuracy: 0.8426\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6744 - accuracy: 0.7979\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5265 - accuracy: 0.8429\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6731 - accuracy: 0.7984\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5352 - accuracy: 0.8438\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6704 - accuracy: 0.7989\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5291 - accuracy: 0.8440\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6660 - accuracy: 0.8009\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5418 - accuracy: 0.8370\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6689 - accuracy: 0.7995\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5327 - accuracy: 0.8421\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6707 - accuracy: 0.8001\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5476 - accuracy: 0.8392\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6674 - accuracy: 0.8017\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5186 - accuracy: 0.8482\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2296 - accuracy: 0.5914\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2184 - accuracy: 0.6318\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.1210 - accuracy: 0.6341\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0177 - accuracy: 0.6822\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2882 - accuracy: 0.5855\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.1276 - accuracy: 0.6366\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1402 - accuracy: 0.6360\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0665 - accuracy: 0.6661\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.1991 - accuracy: 0.6192\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0224 - accuracy: 0.6910\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2020 - accuracy: 0.6241\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.3231 - accuracy: 0.6134\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2058 - accuracy: 0.6234\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1279 - accuracy: 0.6737\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2081 - accuracy: 0.6207\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2043 - accuracy: 0.6359\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1674 - accuracy: 0.6412\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.0038 - accuracy: 0.6812\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2294 - accuracy: 0.6245\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2318 - accuracy: 0.6224\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2426 - accuracy: 0.6214\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2159 - accuracy: 0.6422\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2056 - accuracy: 0.6251\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0302 - accuracy: 0.6833\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2916 - accuracy: 0.6111\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1872 - accuracy: 0.6600\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3086 - accuracy: 0.6129\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4278 - accuracy: 0.6399\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.3302 - accuracy: 0.6127\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2970 - accuracy: 0.6284\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2524 - accuracy: 0.6242\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1464 - accuracy: 0.6572\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4159 - accuracy: 0.5976\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.4593 - accuracy: 0.6192\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2699 - accuracy: 0.6190\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1524 - accuracy: 0.6537\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.4002 - accuracy: 0.5991\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3212 - accuracy: 0.6374\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.3483 - accuracy: 0.6032\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2945 - accuracy: 0.6226\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.1743 - accuracy: 0.6415\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0033 - accuracy: 0.6838\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.5008 - accuracy: 0.6045\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4976 - accuracy: 0.5987\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.3901 - accuracy: 0.6080\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3922 - accuracy: 0.6113\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1778 - accuracy: 0.6376\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0475 - accuracy: 0.6719\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5893 - accuracy: 0.5918\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.6753 - accuracy: 0.5673\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2491 - accuracy: 0.6193\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1722 - accuracy: 0.5992\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2716 - accuracy: 0.6179\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.4661 - accuracy: 0.5971\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2998 - accuracy: 0.6148\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1801 - accuracy: 0.6489\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.5585 - accuracy: 0.5956\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4122 - accuracy: 0.6567\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 1.3516 - accuracy: 0.6096\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5041 - accuracy: 0.6277\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.5238 - accuracy: 0.6023\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2470 - accuracy: 0.6830\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6537 - accuracy: 0.5897\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.3733 - accuracy: 0.5676\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.5998 - accuracy: 0.5850\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4780 - accuracy: 0.6252\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.6128 - accuracy: 0.5888\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.6268 - accuracy: 0.6155\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9292 - accuracy: 0.5765\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7051 - accuracy: 0.6193\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2343 - accuracy: 0.6312\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1525 - accuracy: 0.6632\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.7409 - accuracy: 0.5832\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5133 - accuracy: 0.6403\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.7326 - accuracy: 0.5607\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 3.7277 - accuracy: 0.4394\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2293 - accuracy: 0.6282\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.1467 - accuracy: 0.6355\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0651 - accuracy: 0.5680\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.9337 - accuracy: 0.5269\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.5326 - accuracy: 0.5903\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3131 - accuracy: 0.6489\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.7744 - accuracy: 0.5762\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7564 - accuracy: 0.6030\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6097 - accuracy: 0.5815\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.7639 - accuracy: 0.5616\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6974 - accuracy: 0.5946\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.6060 - accuracy: 0.6205\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.5573 - accuracy: 0.5912\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3486 - accuracy: 0.5959\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7988 - accuracy: 0.7588\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.7218 - accuracy: 0.7782\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7847 - accuracy: 0.7604\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6777 - accuracy: 0.7984\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7992 - accuracy: 0.7570\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6960 - accuracy: 0.7892\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7362 - accuracy: 0.7754\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6294 - accuracy: 0.8110\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7321 - accuracy: 0.7763\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6481 - accuracy: 0.8009\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7297 - accuracy: 0.7761\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6244 - accuracy: 0.8105\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7196 - accuracy: 0.7792\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6131 - accuracy: 0.8150\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7094 - accuracy: 0.7802\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6446 - accuracy: 0.8048\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7186 - accuracy: 0.7781\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6254 - accuracy: 0.8086\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7030 - accuracy: 0.7837\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6016 - accuracy: 0.8179\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7030 - accuracy: 0.7842\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6243 - accuracy: 0.8120\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7025 - accuracy: 0.7846\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6162 - accuracy: 0.8126\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6952 - accuracy: 0.7854\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6201 - accuracy: 0.8121\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6964 - accuracy: 0.7839\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5923 - accuracy: 0.8211\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.7038 - accuracy: 0.7847\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6057 - accuracy: 0.8183\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6908 - accuracy: 0.7874\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6458 - accuracy: 0.7988\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6943 - accuracy: 0.7858\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5886 - accuracy: 0.8224\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6897 - accuracy: 0.7869\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.5929 - accuracy: 0.8195\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6872 - accuracy: 0.7887\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5921 - accuracy: 0.8196\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6870 - accuracy: 0.7873\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6362 - accuracy: 0.8032\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6868 - accuracy: 0.7881\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5916 - accuracy: 0.8174\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6869 - accuracy: 0.7877\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6315 - accuracy: 0.8054\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6885 - accuracy: 0.7880\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5853 - accuracy: 0.8242\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6909 - accuracy: 0.7885\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5973 - accuracy: 0.8165\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6850 - accuracy: 0.7897\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5727 - accuracy: 0.8284\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6868 - accuracy: 0.7896\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6180 - accuracy: 0.8166\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6912 - accuracy: 0.7868\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6015 - accuracy: 0.8142\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6862 - accuracy: 0.7888\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6312 - accuracy: 0.8121\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6822 - accuracy: 0.7899\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6073 - accuracy: 0.8212\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6895 - accuracy: 0.7869\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6382 - accuracy: 0.8048\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6907 - accuracy: 0.7863\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6310 - accuracy: 0.8050\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6889 - accuracy: 0.7889\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6017 - accuracy: 0.8195\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6855 - accuracy: 0.7905\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5890 - accuracy: 0.8195\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6955 - accuracy: 0.7859\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6300 - accuracy: 0.8046\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6879 - accuracy: 0.7895\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6728 - accuracy: 0.7877\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6924 - accuracy: 0.7876\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6310 - accuracy: 0.8045\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6911 - accuracy: 0.7878\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5988 - accuracy: 0.8189\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6818 - accuracy: 0.7904\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6210 - accuracy: 0.8150\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6958 - accuracy: 0.7854\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6702 - accuracy: 0.8021\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6902 - accuracy: 0.7867\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6103 - accuracy: 0.8149\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6933 - accuracy: 0.7859\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6043 - accuracy: 0.8180\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6901 - accuracy: 0.7893\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6025 - accuracy: 0.8208\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6907 - accuracy: 0.7887\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6103 - accuracy: 0.8212\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6907 - accuracy: 0.7885\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6112 - accuracy: 0.8064\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6884 - accuracy: 0.7894\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5839 - accuracy: 0.8234\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 1.0756 - accuracy: 0.6909\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.8327 - accuracy: 0.7571\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.0428 - accuracy: 0.7004\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.8090 - accuracy: 0.7631\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.0331 - accuracy: 0.7066\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.8129 - accuracy: 0.7544\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.9439 - accuracy: 0.7271\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7512 - accuracy: 0.7802\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.9347 - accuracy: 0.7273\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7461 - accuracy: 0.7796\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.9393 - accuracy: 0.7266\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7600 - accuracy: 0.7728\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8973 - accuracy: 0.7389\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.7288 - accuracy: 0.7843\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9044 - accuracy: 0.7326\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7350 - accuracy: 0.7836\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8882 - accuracy: 0.7404\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7211 - accuracy: 0.7893\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8692 - accuracy: 0.7459\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7043 - accuracy: 0.7948\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8759 - accuracy: 0.7415\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7143 - accuracy: 0.7928\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8775 - accuracy: 0.7407\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7103 - accuracy: 0.7917\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8538 - accuracy: 0.7485\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7015 - accuracy: 0.7930\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8549 - accuracy: 0.7467\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7020 - accuracy: 0.7945\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8582 - accuracy: 0.7467\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7013 - accuracy: 0.7894\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8471 - accuracy: 0.7496\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6983 - accuracy: 0.7912\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8463 - accuracy: 0.7486\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6907 - accuracy: 0.7999\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8522 - accuracy: 0.7489\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6923 - accuracy: 0.7936\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8514 - accuracy: 0.7470\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6901 - accuracy: 0.7984\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8377 - accuracy: 0.7512\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7121 - accuracy: 0.7878\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8465 - accuracy: 0.7502\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6873 - accuracy: 0.7963\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8359 - accuracy: 0.7529\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6818 - accuracy: 0.7998\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8322 - accuracy: 0.7531\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6848 - accuracy: 0.7995\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8316 - accuracy: 0.7534\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.7943\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8313 - accuracy: 0.7532\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6792 - accuracy: 0.7992\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8346 - accuracy: 0.7517\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7013 - accuracy: 0.7923\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8318 - accuracy: 0.7526\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6826 - accuracy: 0.7991\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8223 - accuracy: 0.7571\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6770 - accuracy: 0.7998\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8271 - accuracy: 0.7529\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6878 - accuracy: 0.7983\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8261 - accuracy: 0.7555\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6804 - accuracy: 0.7975\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8237 - accuracy: 0.7537\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6732 - accuracy: 0.8013\n",
            "1563/1563 [==============================] - 7s 3ms/step - loss: 0.8239 - accuracy: 0.7552\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6825 - accuracy: 0.8004\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8204 - accuracy: 0.7575\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6777 - accuracy: 0.7982\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8200 - accuracy: 0.7545\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6945 - accuracy: 0.7934\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8202 - accuracy: 0.7552\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6645 - accuracy: 0.8054\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8197 - accuracy: 0.7558\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6759 - accuracy: 0.8017\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8186 - accuracy: 0.7563\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 0.6731 - accuracy: 0.8007\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8209 - accuracy: 0.7544\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6655 - accuracy: 0.8060\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8175 - accuracy: 0.7569\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6746 - accuracy: 0.7962\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8170 - accuracy: 0.7562\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6874 - accuracy: 0.7926\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8133 - accuracy: 0.7567\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6761 - accuracy: 0.7943\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8163 - accuracy: 0.7573\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6655 - accuracy: 0.8018\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8148 - accuracy: 0.7562\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6593 - accuracy: 0.8057\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8139 - accuracy: 0.7578\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6868 - accuracy: 0.7906\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8125 - accuracy: 0.7569\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6636 - accuracy: 0.8032\n",
            "2344/2344 [==============================] - 8s 3ms/step - loss: 0.6161 - accuracy: 0.8162\n",
            "Best: 0.8431600133577982 using {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.20177333056926727, Stdev: 0.08858202506055823 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.21289333701133728, Stdev: 0.03010775736077516 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.270906666914622, Stdev: 0.004044068008254766 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.2942666709423065, Stdev: 0.019865826992399523 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.3049600025018056, Stdev: 0.04671298578211674 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.26124000549316406, Stdev: 0.034545573784046255 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.272626668214798, Stdev: 0.05307529108678381 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.2796933303276698, Stdev: 0.049188009226956785 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.30321333805720013, Stdev: 0.03882518472540958 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.2462799996137619, Stdev: 0.033749997154625885 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.23593333860238394, Stdev: 0.04654972133783599 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.2988133331139882, Stdev: 0.05902853302322334 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.2905333340167999, Stdev: 0.020879699326361422 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.3197466731071472, Stdev: 0.023042375160218474 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.2633200039466222, Stdev: 0.04935942946855235 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.7890933354695638, Stdev: 0.0053558371539968175 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.7982133428255717, Stdev: 0.004536830795956102 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.7990799943606058, Stdev: 0.006988542448809679 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8077600002288818, Stdev: 0.0029188071159800284 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.8052666584650675, Stdev: 0.004997300966223058 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.8019066651662191, Stdev: 0.005313262240153626 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.8014799952507019, Stdev: 0.0028957016366604493 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8033333420753479, Stdev: 0.0017676759932315816 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.802133321762085, Stdev: 0.007057059472982729 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.7950666546821594, Stdev: 0.0088457175563595 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.7996400197347006, Stdev: 0.008097475449912225 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.7975200017293295, Stdev: 0.001648580075751109 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.7953333258628845, Stdev: 0.004151828197990898 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.7940933505694071, Stdev: 0.0015974433391661915 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8064266641934713, Stdev: 0.0035881735069429395 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.787880003452301, Stdev: 0.002691017517185283 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.8085733453432719, Stdev: 0.002827352054731836 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.8169866800308228, Stdev: 0.0015517069265162313 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.8243066668510437, Stdev: 0.0009181810745485414 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.829479992389679, Stdev: 0.002381488603592076 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.8283066550890604, Stdev: 0.002003270855013503 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.8352133433024088, Stdev: 0.0013949657560067808 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.8385333220163981, Stdev: 0.004129174829763818 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.8350133299827576, Stdev: 0.0025250292983040163 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.8392666776974996, Stdev: 0.004234379708528774 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.8351333340009054, Stdev: 0.005063987572452129 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8413333296775818, Stdev: 0.003779276774737746 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.8428133328755697, Stdev: 0.00012365976650181648 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8416133324305216, Stdev: 0.00323584848820124 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8431600133577982, Stdev: 0.0037527626640169124 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.6502266724904379, Stdev: 0.022720105116205324 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.6568266550699869, Stdev: 0.032370427630180894 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.6636000076929728, Stdev: 0.019812038199577397 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.6492666602134705, Stdev: 0.025373083950544027 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.6427600185076395, Stdev: 0.013060394306710608 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.6433866620063782, Stdev: 0.01713328145022041 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.6479466756184896, Stdev: 0.026034657193056455 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.6273200114568075, Stdev: 0.031953709891913644 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.5878533323605856, Stdev: 0.014573269880338609 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.6444533268610636, Stdev: 0.012253248435888613 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.6252666711807251, Stdev: 0.047144538516632556 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.6326666673024496, Stdev: 0.021647179925659538 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.5717333257198334, Stdev: 0.09362259169877976 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.5929466684659322, Stdev: 0.05033120304319718 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.5926533142725626, Stdev: 0.024178220165517177 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.7886266708374023, Stdev: 0.008241687797718287 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.807479997475942, Stdev: 0.004671718112019043 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.8094800114631653, Stdev: 0.004173941538407457 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8141599893569946, Stdev: 0.0026418088493773196 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.8171600103378296, Stdev: 0.003774576616393186 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.8135599891344706, Stdev: 0.01050109582875951 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.8133866588274637, Stdev: 0.007260868491387359 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8153599898020426, Stdev: 0.0077158026104939695 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.819706658522288, Stdev: 0.0061942390951857575 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8127333521842957, Stdev: 0.006709295271843047 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.8146800001462301, Stdev: 0.006816527707475548 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.7989200154940287, Stdev: 0.007919656845972681 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.8119733333587646, Stdev: 0.007176374773373808 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.8178933461507162, Stdev: 0.0024009621916417107 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8170000116030375, Stdev: 0.007575121466980059 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.7581866780916849, Stdev: 0.0036289631940289967 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.7775466839472452, Stdev: 0.003366545878316956 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.7857199907302856, Stdev: 0.0025363546671830594 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.7930799921353658, Stdev: 0.0012728992758218093 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.7922933300336202, Stdev: 0.002142743499253114 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.7948933442433676, Stdev: 0.003651111140310771 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.7941466768582662, Stdev: 0.004598231686016889 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.7978533307711283, Stdev: 0.0025286301383884106 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.7968666752179464, Stdev: 0.003215017114717642 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.7985599835713705, Stdev: 0.000962214800481698 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.7999466856320699, Stdev: 0.0013134537554186062 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8001600106557211, Stdev: 0.005047410135053223 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.8009599844614664, Stdev: 0.0039731209015599084 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.7962533235549927, Stdev: 0.004012304919841757 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.7997999986012777, Stdev: 0.006614158469484576 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 480}\n"
          ]
        }
      ],
      "source": [
        "# save start time \n",
        "start = time()\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=hyper_parameters, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# save end time \n",
        "end = time()\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THKMZLNv7mNw",
        "outputId": "bedc1768-baa5-412a-a17a-330a3e65fd1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "45.96631329854329"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# total run time \n",
        "total_run_time_in_minutes = (end - start)/60\n",
        "total_run_time_in_minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XgJsrZb7mNx",
        "outputId": "37a6d5f1-2313-43a7-bb0b-ce668d402366"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'activation': 'relu', 'learning_rate': 0.001, 'units': 480}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_result.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYufbSI87mNx",
        "outputId": "d8cecb87-dbcc-4df4-f550-1ddbb84fd79b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4893 - accuracy: 0.8553\n"
          ]
        }
      ],
      "source": [
        "# because all other optimization approaches are reporting test set score\n",
        "# let's calculate the test set score in this case \n",
        "best_model = grid_result.best_estimator_\n",
        "test_acc = best_model.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlR-pVwP7mNx",
        "outputId": "9fe11fe6-d493-4b39-f62b-cffab5efe6a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8552799820899963"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj4jJ0Qm7mNx"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparameter combination and model score. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "10px3N2q7mNx",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9577db883482c6cded3836e5cfbf5a74",
          "grade": true,
          "grade_id": "cell-eb06d682d2790f6e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "The best performing hyperparameter combination score for\n",
        "\n",
        "   > units: 480\n",
        "\n",
        "   > learning_rate : 0.001\n",
        "\n",
        "   > activation : relu\n",
        "\n",
        "   > score : 85.53%\n",
        "\n",
        "The total runtime for Grid Search taking `all sample combinations` is `45m 58s`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu-lBWph7mNq"
      },
      "source": [
        "------\n",
        "## 2.2 Random Search with `keras-tuner`\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `RandomSearch` tuner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgzUq6joR0Uv"
      },
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complied keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(hp.get('learning_rate')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GjK2oeBwSNwZ",
        "outputId": "528c3635-ec5b-4596-9647-571ffe5c8da1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'relu'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hp = HyperParameters()\n",
        "hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
        "hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "id": "8DApqLli7mNq",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aaff9aae33845f374e15f2381719d83a",
          "grade": false,
          "grade_id": "cell-8c1dfb9b6d12bea2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "bb2e15a4-4549-42a0-d1d4-b983c88b131e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "96"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# how many unique hyperparameter combinations do we have? \n",
        "# HINT: take the product of the number of possible values for each hyperparameter \n",
        "# save your answer to n_unique_hparam_combos\n",
        "\n",
        "# YOUR CODE HERE\n",
        "n_values_in_units = 512/32\n",
        "n_values_in_learning = 3 \n",
        "n_values_in_activation = 2 \n",
        "\n",
        "n_unique_hparam_combos = int(n_values_in_units * n_values_in_learning * n_values_in_activation)\n",
        "n_unique_hparam_combos\n",
        "# raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "id": "m1UKRA597mNq",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a9d628451e83431e1b52da10eccf2c00",
          "grade": false,
          "grade_id": "cell-1fa83950bb2d5f92",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "94fde6ae-2621-45d2-ba4f-95f7360ac1da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# how many of these do we want to randomly sample?\n",
        "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
        "# save this number to n_param_combos_to_sample\n",
        "\n",
        "# YOUR CODE HERE\n",
        "fraction_to_sample = 0.25\n",
        "n_param_combos_to_sample = int(n_unique_hparam_combos * fraction_to_sample)\n",
        "n_param_combos_to_sample\n",
        "# raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TzaNnzoQU4U"
      },
      "source": [
        "### Instantiate a `RandomSearch()` object for your grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9PCHLBWQPcb"
      },
      "outputs": [],
      "source": [
        "random_tuner = RandomSearch(\n",
        "            build_model,\n",
        "            objective='val_accuracy',\n",
        "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model \n",
        "            seed=1234,\n",
        "            hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "            directory='./keras-tuner-trial',\n",
        "            project_name='random_search')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGFdv1qE7mNr",
        "outputId": "a6763870-d349-4a7d-b730-e50d84a61168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 24 Complete [00h 00m 43s]\n",
            "val_accuracy: 0.707040011882782\n",
            "\n",
            "Best val_accuracy So Far: 0.8754799962043762\n",
            "Total elapsed time: 00h 15m 00s\n"
          ]
        }
      ],
      "source": [
        " # take note of Total elapsed time in print out -- took ~10 minutes without GPU\n",
        "random_tuner.search(X_train, y_train,\n",
        "                    epochs=3,\n",
        "                    validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNBUhIe97mNr",
        "outputId": "74f49267-4a1d-44e8-ce87-c667dd5ce744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/random_search\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7f19a0384940>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 416\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8754799962043762\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8708400130271912\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 288\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8700000047683716\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8676400184631348\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8654400110244751\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8551999926567078\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 96\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8438000082969666\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8357999920845032\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 448\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8335599899291992\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.01\n",
            "activation: relu\n",
            "Score: 0.826200008392334\n"
          ]
        }
      ],
      "source": [
        "# identify the best score and hyperparamter (should be at the top since scores are ranked)\n",
        "random_tuner.results_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRpQVXBE7mNr",
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparameter combination and model score. \n",
        "Note that because this is Random Search, multiple runs might have slighly different outcomes. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "aQjMc84c7mNs",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f084b5d373f8589a1de8d6d4473b974a",
          "grade": true,
          "grade_id": "cell-5527738b6382c164",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "\n",
        "The best performing hyperparameter combination score for\n",
        "\n",
        "   > units: 416\n",
        "\n",
        "   > learning_rate : 0.001\n",
        "\n",
        "   > activation : relu\n",
        "\n",
        "   > score : 87.55%\n",
        "\n",
        "The total runtime for Random Search when taking `24 samples` is `15m 00s`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXjW7eYA7mNs"
      },
      "source": [
        "------\n",
        "## 2.3 Bayesian Optimization with `keras-tuner`\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `BayesianOptimization` tuner.\n",
        "\n",
        "Pay special attention to these `BayesianOptimization` parameters: `num_initial_points` and `beta`. \n",
        "\n",
        "`num_initial_points`: \n",
        "\n",
        "Number of randomly selected hyperparameter combinations to try before applying bayesian probability to determine likelihood of which param combo to try next, based on expected improvement.\n",
        "\n",
        "\n",
        "`beta`: \n",
        "\n",
        "Larger values means more willing to explore new hyperparameter combinations (analogous to searching for the global minimum in Gradient Descent); smaller values means that it is less willing to try new hyperparameter combinations (analogous to getting stuck in a local minimum in Gradient Descent). \n",
        "\n",
        "As a start, error on the side of larger values. What defines a small or large value you ask? That question would pull us into the mathematical intricacies of Bayesian Optimization and Gaussian Processes. For simplicity, notice that the default value is 2.6 and work from there. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NXjQBn47mNs"
      },
      "outputs": [],
      "source": [
        "# we know that 24 samples is about 25% of 96 possible hyper-parameter combos\n",
        "# let's set up a run with the same parameters we used for RandomSearch() so the comparison will be apples-to-apples\n",
        "# feel free to play with any of these numbers later\n",
        "max_trials=24\n",
        "num_initial_points=5\n",
        "beta=5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhZNIJZ4RS5Y"
      },
      "source": [
        "#### Instantiate a `BayesianOptimization()` object for your grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33joO_J97mNs"
      },
      "outputs": [],
      "source": [
        "bayesian_tuner = BayesianOptimization(\n",
        "                    build_model,\n",
        "                    objective='val_accuracy',\n",
        "                    max_trials=max_trials,\n",
        "                    hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "                    num_initial_points=num_initial_points, \n",
        "                    beta=beta, \n",
        "                    seed=1234,\n",
        "                    directory='./keras-tuner-trial',\n",
        "                    project_name='bayesian_optimization_4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9AM5Pdj7mNt",
        "outputId": "7e822c83-b4c2-4c30-f98c-df57720c2d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 24 Complete [00h 00m 29s]\n",
            "val_accuracy: 0.8742799758911133\n",
            "\n",
            "Best val_accuracy So Far: 0.8762800097465515\n",
            "Total elapsed time: 00h 15m 10s\n"
          ]
        }
      ],
      "source": [
        "bayesian_tuner.search(X_train, y_train,\n",
        "               epochs=3,\n",
        "               validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJcHC8d87mNt",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "364bb22f-599d-4b98-b20f-47fd79b105f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/bayesian_optimization_4\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7f18e5555130>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8762800097465515\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8756399750709534\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 448\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8751199841499329\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 320\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8746399879455566\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8742799758911133\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8733199834823608\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8733199834823608\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8703200221061707\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.870199978351593\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 160\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8650400042533875\n"
          ]
        }
      ],
      "source": [
        "bayesian_tuner.results_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woo9D9AU7mNu"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparameter combination and model score. \n",
        "Note that because this is  Bayesian Optimization, multiple runs might have slighly different outcomes. \n",
        " \n",
        " "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "1EXa47mH7mNu",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1badcdca408cdd49bc2e409dca3bac5a",
          "grade": true,
          "grade_id": "cell-ff95600bf745f40f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "The best performing hyperparameter combination score for\n",
        "\n",
        "   > units: 512\n",
        "\n",
        "   > learning_rate : 0.001\n",
        "\n",
        "   > activation : relu\n",
        "\n",
        "   > score : 87.63%\n",
        "\n",
        "The total runtime for Bayesian Optimization when taking `24 samples` is `15m 10s`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOZ5-tJDraFE"
      },
      "source": [
        "We should point out that Gridsearch split the training set internally and created a test set, whereas keras-tuner allows us to pass in a test set. This means that the keras-tuner algorithms were using one test set and our sklearn GridSearchCV was using a different test set - so this isn't a perfectly exact 1-to-1 comparison but it'll have to do. In order to compensate for this, we did score the best model on the same test set that keras-tuner used. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPYChhrC7mNx"
      },
      "source": [
        "_______\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "The spirit of this experiment is to expose you to the idea of benchmarking and comparing the trade-offs of various gridsearch approaches. \n",
        "\n",
        "Even if we did find a way to pass in the original test set into GridSearchCV, we can see that both Random Search and Bayesian Optimization are arguably better alternatives to a brute force grid search when we consider the trade-offs of run time and locating the best performing model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sth1AfwX7mNy"
      },
      "source": [
        "----\n",
        "\n",
        "# Stretch Goals\n",
        "\n",
        "- Feel free to run whatever gridsearch experiments on whatever models you like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2APQG9H7mNy"
      },
      "outputs": [],
      "source": [
        "# this is your open playground - be free to explore as you wish "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
